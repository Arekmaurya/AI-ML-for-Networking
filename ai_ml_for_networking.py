# -*- coding: utf-8 -*-
"""AI/ML for Networking

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GUZG3kE9mERoykJWgv55l0lUmJf_PSJK
"""

#!/usr/bin/env python3
"""
AI Network Traffic Detection System
Complete implementation for network traffic classification using Random Forest and LSTM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import warnings
warnings.filterwarnings('ignore')

class NetworkTrafficDetector:
    """
    Main class for network traffic detection using AI/ML techniques
    """

    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.rf_model = None
        self.lstm_model = None
        self.feature_columns = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {self.device}")

    def load_and_preprocess_data(self, file_path, sample_size=None):
        """
        Load and preprocess network traffic data
        """
        print("Loading network traffic data...")

        # Load data
        try:
            df = pd.read_csv(file_path)
        except Exception as e:
            print(f"Error loading data: {e}")
            # Create sample data if file not found
            df = self.create_sample_data()

        # Sample data if specified
        if sample_size and len(df) > sample_size:
            df = df.sample(n=sample_size, random_state=42)

        print(f"Dataset shape: {df.shape}")
        print(f"Columns: {df.columns.tolist()}")

        # Basic preprocessing
        df = self.clean_data(df)

        return df

    def create_sample_data(self):
        """
        Create sample network traffic data for demonstration
        """
        print("Creating sample network traffic data...")

        np.random.seed(42)
        n_samples = 10000

        # Create synthetic network traffic features
        data = {
            'Flow_Duration': np.random.exponential(100000, n_samples),
            'Total_Fwd_Packets': np.random.poisson(50, n_samples),
            'Total_Backward_Packets': np.random.poisson(30, n_samples),
            'Total_Length_of_Fwd_Packets': np.random.exponential(5000, n_samples),
            'Total_Length_of_Bwd_Packets': np.random.exponential(3000, n_samples),
            'Fwd_Packet_Length_Max': np.random.exponential(1500, n_samples),
            'Fwd_Packet_Length_Min': np.random.exponential(50, n_samples),
            'Fwd_Packet_Length_Mean': np.random.exponential(500, n_samples),
            'Bwd_Packet_Length_Max': np.random.exponential(1500, n_samples),
            'Bwd_Packet_Length_Min': np.random.exponential(50, n_samples),
            'Flow_Bytes_per_s': np.random.exponential(10000, n_samples),
            'Flow_Packets_per_s': np.random.exponential(100, n_samples),
            'Flow_IAT_Mean': np.random.exponential(1000, n_samples),
            'Flow_IAT_Std': np.random.exponential(500, n_samples),
            'Flow_IAT_Max': np.random.exponential(5000, n_samples),
            'Flow_IAT_Min': np.random.exponential(10, n_samples),
            'Fwd_IAT_Mean': np.random.exponential(1000, n_samples),
            'Fwd_IAT_Std': np.random.exponential(500, n_samples),
            'Bwd_IAT_Mean': np.random.exponential(1000, n_samples),
            'Bwd_IAT_Std': np.random.exponential(500, n_samples),
            'Fwd_PSH_Flags': np.random.binomial(1, 0.3, n_samples),
            'Bwd_PSH_Flags': np.random.binomial(1, 0.2, n_samples),
            'Fwd_URG_Flags': np.random.binomial(1, 0.01, n_samples),
            'Bwd_URG_Flags': np.random.binomial(1, 0.01, n_samples),
            'FIN_Flag_Count': np.random.binomial(2, 0.5, n_samples),
            'SYN_Flag_Count': np.random.binomial(2, 0.3, n_samples),
            'RST_Flag_Count': np.random.binomial(2, 0.1, n_samples),
            'PSH_Flag_Count': np.random.binomial(2, 0.4, n_samples),
            'ACK_Flag_Count': np.random.binomial(2, 0.8, n_samples),
            'URG_Flag_Count': np.random.binomial(2, 0.05, n_samples),
            'Down_Up_Ratio': np.random.uniform(0, 10, n_samples),
            'Average_Packet_Size': np.random.exponential(500, n_samples),
            'Avg_Fwd_Segment_Size': np.random.exponential(400, n_samples),
            'Avg_Bwd_Segment_Size': np.random.exponential(300, n_samples),
            'Subflow_Fwd_Packets': np.random.poisson(25, n_samples),
            'Subflow_Bwd_Packets': np.random.poisson(15, n_samples),
            'Init_Win_bytes_forward': np.random.exponential(8000, n_samples),
            'Init_Win_bytes_backward': np.random.exponential(8000, n_samples),
            'Active_Mean': np.random.exponential(100000, n_samples),
            'Active_Std': np.random.exponential(50000, n_samples),
            'Active_Max': np.random.exponential(200000, n_samples),
            'Active_Min': np.random.exponential(1000, n_samples),
            'Idle_Mean': np.random.exponential(50000, n_samples),
            'Idle_Std': np.random.exponential(25000, n_samples),
            'Idle_Max': np.random.exponential(100000, n_samples),
            'Idle_Min': np.random.exponential(100, n_samples),
        }

        # Create labels with realistic distribution
        labels = np.random.choice(['BENIGN', 'DDoS', 'PortScan', 'Bot', 'Infiltration', 'Web Attack', 'Brute Force'],
                                n_samples, p=[0.7, 0.1, 0.05, 0.05, 0.03, 0.04, 0.03])

        data['Label'] = labels

        return pd.DataFrame(data)

    def clean_data(self, df):
        """
        Clean and preprocess the data
        """
        print("Cleaning data...")

        # Handle infinite values
        df = df.replace([np.inf, -np.inf], np.nan)

        # Handle missing values
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].median())

        # Handle categorical missing values
        categorical_columns = df.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')

        # Remove duplicates
        df = df.drop_duplicates()

        print(f"Cleaned dataset shape: {df.shape}")
        return df

    def prepare_features(self, df, target_column='Label'):
        """
        Prepare features for model training
        """
        print("Preparing features...")

        # Separate features and target
        X = df.drop(columns=[target_column])
        y = df[target_column]

        # Store feature columns
        self.feature_columns = X.columns.tolist()

        # Encode categorical features
        categorical_columns = X.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col])

        # Scale features
        X_scaled = self.scaler.fit_transform(X)
        X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

        # Encode target labels
        y_encoded = self.label_encoder.fit_transform(y)

        print(f"Features shape: {X_scaled.shape}")
        print(f"Target classes: {self.label_encoder.classes_}")

        return X_scaled, y_encoded

    def train_random_forest(self, X_train, y_train, X_test, y_test):
        """
        Train Random Forest model
        """
        print("\nTraining Random Forest model...")

        # Initialize Random Forest
        self.rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=20,
            random_state=42,
            n_jobs=-1
        )

        # Train model
        self.rf_model.fit(X_train, y_train)

        # Make predictions
        y_pred = self.rf_model.predict(X_test)

        # Evaluate model
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted')
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        print(f"Random Forest Results:")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-score: {f1:.4f}")

        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': self.feature_columns,
            'importance': self.rf_model.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\nTop 10 Important Features:")
        print(feature_importance.head(10))

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'predictions': y_pred,
            'feature_importance': feature_importance
        }

    def create_sequences(self, X, y, sequence_length=10):
        """
        Create sequences for LSTM model
        """
        sequences = []
        labels = []

        for i in range(len(X) - sequence_length + 1):
            sequences.append(X.iloc[i:i+sequence_length].values)
            labels.append(y[i+sequence_length-1])

        return np.array(sequences), np.array(labels)

    def train_lstm(self, X_train, y_train, X_test, y_test, sequence_length=10):
        """
        Train LSTM model
        """
        print("\nTraining LSTM model...")

        # Create sequences
        X_train_seq, y_train_seq = self.create_sequences(X_train, y_train, sequence_length)
        X_test_seq, y_test_seq = self.create_sequences(X_test, y_test, sequence_length)

        # Convert to PyTorch tensors
        X_train_tensor = torch.FloatTensor(X_train_seq).to(self.device)
        y_train_tensor = torch.LongTensor(y_train_seq).to(self.device)
        X_test_tensor = torch.FloatTensor(X_test_seq).to(self.device)
        y_test_tensor = torch.LongTensor(y_test_seq).to(self.device)

        # Create data loaders
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

        # Define LSTM model
        class LSTMModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, num_classes):
                super(LSTMModel, self).__init__()
                self.hidden_size = hidden_size
                self.num_layers = num_layers
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
                self.fc = nn.Linear(hidden_size, num_classes)
                self.dropout = nn.Dropout(0.2)

            def forward(self, x):
                h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
                c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

                out, _ = self.lstm(x, (h0, c0))
                out = self.dropout(out[:, -1, :])
                out = self.fc(out)
                return out

        # Initialize model
        input_size = X_train_seq.shape[2]
        hidden_size = 128
        num_layers = 2
        num_classes = len(np.unique(y_train))

        self.lstm_model = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(self.device)

        # Loss and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.lstm_model.parameters(), lr=0.001)

        # Training loop
        num_epochs = 10
        train_losses = []

        for epoch in range(num_epochs):
            self.lstm_model.train()
            epoch_loss = 0

            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = self.lstm_model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()

            avg_loss = epoch_loss / len(train_loader)
            train_losses.append(avg_loss)

            if (epoch + 1) % 2 == 0:
                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')

        # Evaluate LSTM model
        self.lstm_model.eval()
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch_X, batch_y in test_loader:
                outputs = self.lstm_model(batch_X)
                _, predicted = torch.max(outputs.data, 1)
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(batch_y.cpu().numpy())

        # Calculate metrics
        accuracy = accuracy_score(all_labels, all_predictions)
        precision = precision_score(all_labels, all_predictions, average='weighted')
        recall = recall_score(all_labels, all_predictions, average='weighted')
        f1 = f1_score(all_labels, all_predictions, average='weighted')

        print(f"\nLSTM Results:")
        print(f"Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-score: {f1:.4f}")

        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'predictions': all_predictions,
            'true_labels': all_labels,
            'train_losses': train_losses
        }

    def plot_results(self, rf_results, lstm_results):
        """
        Plot comparison results
        """
        print("\nGenerating result plots...")

        # Model comparison
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        rf_scores = [rf_results[metric] for metric in metrics]
        lstm_scores = [lstm_results[metric] for metric in metrics]

        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Model comparison
        axes[0, 0].bar(['Random Forest', 'LSTM'], [rf_results['accuracy'], lstm_results['accuracy']],
                       color=['blue', 'red'], alpha=0.7)
        axes[0, 0].set_title('Model Accuracy Comparison')
        axes[0, 0].set_ylabel('Accuracy')
        axes[0, 0].set_ylim(0, 1)

        # Metrics comparison
        x_pos = np.arange(len(metrics))
        width = 0.35

        axes[0, 1].bar(x_pos - width/2, rf_scores, width, label='Random Forest', alpha=0.7)
        axes[0, 1].bar(x_pos + width/2, lstm_scores, width, label='LSTM', alpha=0.7)
        axes[0, 1].set_title('Metrics Comparison')
        axes[0, 1].set_xlabel('Metrics')
        axes[0, 1].set_ylabel('Score')
        axes[0, 1].set_xticks(x_pos)
        axes[0, 1].set_xticklabels(metrics)
        axes[0, 1].legend()
        axes[0, 1].set_ylim(0, 1)

        # Feature importance (top 10)
        if 'feature_importance' in rf_results:
            top_features = rf_results['feature_importance'].head(10)
            axes[1, 0].barh(range(len(top_features)), top_features['importance'])
            axes[1, 0].set_yticks(range(len(top_features)))
            axes[1, 0].set_yticklabels(top_features['feature'])
            axes[1, 0].set_title('Top 10 Feature Importance (Random Forest)')
            axes[1, 0].set_xlabel('Importance')

        # LSTM training loss
        if 'train_losses' in lstm_results:
            axes[1, 1].plot(lstm_results['train_losses'])
            axes[1, 1].set_title('LSTM Training Loss')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Loss')

        plt.tight_layout()
        plt.show()

        # Confusion matrices
        if 'predictions' in rf_results and 'true_labels' in lstm_results:
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))

            # Random Forest confusion matrix
            cm_rf = confusion_matrix(rf_results['true_labels'], rf_results['predictions'])
            sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[0])
            axes[0].set_title('Random Forest Confusion Matrix')
            axes[0].set_xlabel('Predicted')
            axes[0].set_ylabel('Actual')

            # LSTM confusion matrix
            cm_lstm = confusion_matrix(lstm_results['true_labels'], lstm_results['predictions'])
            sns.heatmap(cm_lstm, annot=True, fmt='d', cmap='Reds', ax=axes[1])
            axes[1].set_title('LSTM Confusion Matrix')
            axes[1].set_xlabel('Predicted')
            axes[1].set_ylabel('Actual')

            plt.tight_layout()
            plt.show()

    def run_complete_analysis(self, data_path=None, sample_size=5000):
        """
        Run complete network traffic analysis
        """
        print("=== AI Network Traffic Detection Analysis ===\n")

        # Load and preprocess data
        df = self.load_and_preprocess_data(data_path, sample_size)

        # Prepare features
        X, y = self.prepare_features(df)

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        print(f"Training set size: {len(X_train)}")
        print(f"Test set size: {len(X_test)}")

        # Train Random Forest
        rf_results = self.train_random_forest(X_train, y_train, X_test, y_test)
        rf_results['true_labels'] = y_test

        # Train LSTM
        lstm_results = self.train_lstm(X_train, y_train, X_test, y_test)

        # Plot results
        self.plot_results(rf_results, lstm_results)

        # Summary
        print("\n=== FINAL RESULTS SUMMARY ===")
        print(f"Random Forest - Accuracy: {rf_results['accuracy']:.4f}, F1: {rf_results['f1_score']:.4f}")
        print(f"LSTM - Accuracy: {lstm_results['accuracy']:.4f}, F1: {lstm_results['f1_score']:.4f}")

        return rf_results, lstm_results

def main():
    """
    Main function to run the network traffic detection analysis
    """
    # Initialize detector
    detector = NetworkTrafficDetector()

    # Run analysis
    # You can provide a path to your dataset here
    # detector.run_complete_analysis(data_path='path/to/your/dataset.csv')

    # For demonstration, we'll use synthetic data
    rf_results, lstm_results = detector.run_complete_analysis(sample_size=5000)

    print("\nAnalysis complete!")
    print("Models have been trained and evaluated.")
    print("Check the plots for detailed performance comparison.")

detector = NetworkTrafficDetector()
detector.run_complete_analysis(sample_size=5000)

